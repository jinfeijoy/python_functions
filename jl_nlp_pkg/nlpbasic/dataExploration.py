import numpy as np
import statistics
import nltk
from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt
import pandas as pd
from scipy.spatial.distance import cosine

def print_bow_example(text, dictionary):
    """
    :param dictionary: the dictionary generated by gensim.corpora.Dictionary(processed_docs)
    :return: print bag of words vector frequency
    """
    for i in range(len(text)):
        print("Word {} (\"{}\") appears {} time.".format(text[i][0], dictionary[text[i][0]], text[i][1]))

def get_similarity_cosin(basedata, filterdata, key, doc_key, comp_col, topn_output=10):
    """
    :param basedata: basedata to do comparison
    :param filterdata: compare data to do comparison
    :param key: key used to do left join e.g. ['A'], ['A','B']
    :param doc_key: index key which used to identify rows
    :param comp_col: comparison column e.g.'A'
    :param topn_topics: rows with top n similarity value
    :return: a dataset with row doc key and similarity value
    """
    basedata['comp_col'] = basedata[comp_col]
    filterdata['comp_col'] = filterdata[comp_col]
    merge_data = pd.merge(basedata, filterdata, how='left', left_on=key, right_on=key, suffixes=('_x', '_y'))
    merge_data.fillna(0, inplace=True)
    similarity_val = merge_data.groupby([doc_key]).apply(lambda x: 1 - cosine(x['comp_col_x'], x['comp_col_y']))
    similarity_data = {"doc_key": similarity_val.index, "cosine": similarity_val}
    similarity_data = pd.DataFrame(similarity_data)
    similarity_data = similarity_data.sort_values(by=['cosine'], ascending=False).head(topn_output)
    return similarity_data

def text_length_summary(data, col, measure = 'max'):
    """
    :param data: dataset to be summary
    :param col: column of data that want to be measured: e.g. "review"
    :param measure: 'max', 'min', 'avg', 'median'
    :return:
    """
    measurer = np.vectorize(len)
    if measure == 'max':
        out = measurer(data[col].astype(str)).max(axis=0)
    if measure == 'min':
        out = measurer(data[col].astype(str)).min(axis=0)
    if measure == 'avg':
        out = statistics.mean(measurer(data[col].astype(str)))
    if measure == 'median':
        out = statistics.median(measurer(data[col].astype(str)))
    return out

def get_topn_freq_bow(corpus, topn = 10):
    """
    :param corpus: corpus: a list of list of tokens (generated from textClean.pipeline)
    :return: top n frequent bow list
    """
    processed_docs = corpus
    doc_list = []
    for i in range(len(processed_docs)):
        doc_list.extend(processed_docs[i])
    freq_list = nltk.FreqDist(doc_list)
    topn_freq_list = freq_list.most_common(topn)
    return topn_freq_list

def generate_word_cloud(corpus):
    """
    :param corpus: a list of list of tokens (generated from textClean.pipeline)
    :return: show the word cloud plot
    """
    preprocessed_tokens = corpus
    tokens = []
    for i in range(len(preprocessed_tokens)):
        tokens.extend(preprocessed_tokens[i])
    for i in range(len(tokens)):
        tokens[i] = tokens[i].lower()
    comment_words = ''
    comment_words += " ".join(tokens) + " "
    stopwords = set(STOPWORDS)
    wordcloud = WordCloud(width=800, height=800,
                          background_color='white',
                          stopwords=stopwords,
                          min_font_size=10).generate(comment_words)
    # plot the WordCloud image
    plt.figure(figsize=(8, 8), facecolor=None)
    plt.imshow(wordcloud)
    plt.axis("off")
    plt.tight_layout(pad=0)
    plt.show()



