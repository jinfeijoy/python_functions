import numpy as np
import statistics
import nltk
from PIL import Image
from wordcloud import WordCloud, STOPWORDS,ImageColorGenerator
import matplotlib.pyplot as plt
import pandas as pd
from scipy.spatial.distance import cosine

def print_bow_example(text, dictionary):
    """
    :param dictionary: the dictionary generated by gensim.corpora.Dictionary(processed_docs)
    :return: print bag of words vector frequency
    """
    for i in range(len(text)):
        print("Word {} (\"{}\") appears {} time.".format(text[i][0], dictionary[text[i][0]], text[i][1]))



def get_similarity_cosin(basedata, comparedata, word_col, doc_key, dataformat = 'long',topn=10, filterbase=None, index_is_int = True):
    """
    :param basedata: basedata to do comparison
    :param filterdata: compare data to do comparison
    :param key: key used to do left join e.g. ['A'], ['A','B']
    :param doc_key: index key which used to identify rows
    :param dataformat: whether the input is long table or wide table, long table has column word and value, wide table column name will be words
    :param topn: rows with top n similarity value
    :param filterbase: base or compare or None, to get top n list based on base table unique value or compare table unique value or combination unique value
    :return: a dataset with row doc key and similarity value
    """
    if dataformat == 'long':
        base = basedata.copy()
        base['dataid'] = 'base'
        compare = comparedata.copy()
        compare['dataid'] = 'compare'
        pre_tmp = base.append(compare)

        joinT = pre_tmp.pivot(index=[doc_key, 'dataid'], columns=word_col).fillna(0).reset_index(level=[0, 1])
        joinT.columns = [doc_key, 'dataid'] + [i[1] for i in joinT.columns][2:]

        baseT = joinT[joinT.dataid == 'base'].drop(columns=['dataid'])
        baseT = baseT.set_index(doc_key)
        compareT = joinT[joinT.dataid == 'compare'].drop(columns=['dataid'])
        compareT = compareT.set_index(doc_key)
    elif dataformat == 'wide':
        baseT = basedata.copy()
        compareT = comparedata.copy()

    baseindex = []
    compareindex = []
    similarity = []
    for i in range(len(baseT)):
        for j in range(len(compareT)):
            baseindex.append(baseT.index[i])
            compareindex.append(compareT.index[j])
            similarity.append(1 - cosine(baseT.iloc[[i]], compareT.iloc[[j]]))

    output = pd.DataFrame({'baseindex': baseindex,
                           'compareindex': compareindex,
                           'similarity': similarity})
    output = output[output.similarity < 1].sort_values(by=['similarity'], ascending=False)  # .head(topn)

    if filterbase == None:
        output['combineid'] = output.baseindex.apply(lambda x: str(x)) + ' ' + output.compareindex.apply(lambda x: str(x))
        if index_is_int == True:
            output['combineid'] = output['combineid'].apply(lambda x: sorted([int(i) for i in x.split(' ')]))
        output = output[~output['combineid'].apply(pd.Series).duplicated()]
        output = output.drop(columns = ['combineid'])
    if filterbase == 'base':
        output = output.drop_duplicates('baseindex')
    elif filterbase == 'compare':
        output = output.drop_duplicates('compareindex')
    output = output.head(topn)

    return output

def text_length_summary(data, col, measure = 'max'):
    """
    :param data: dataset to be summary
    :param col: column of data that want to be measured: e.g. "review"
    :param measure: 'max', 'min', 'avg', 'median'
    :return:
    """
    measurer = np.vectorize(len)
    if measure == 'max':
        out = measurer(data[col].astype(str)).max(axis=0)
    if measure == 'min':
        out = measurer(data[col].astype(str)).min(axis=0)
    if measure == 'avg':
        out = statistics.mean(measurer(data[col].astype(str)))
    if measure == 'median':
        out = statistics.median(measurer(data[col].astype(str)))
    return out

def get_topn_freq_bow(corpus, topn = 10):
    """
    :param corpus: corpus: a list of list of tokens (generated from textClean.pipeline)
    :return: top n frequent bow list
    """
    processed_docs = corpus
    doc_list = []
    for i in range(len(processed_docs)):
        doc_list.extend(processed_docs[i])
    freq_list = nltk.FreqDist(doc_list)
    topn_freq_list = freq_list.most_common(topn)
    return topn_freq_list

def generate_word_cloud(corpus):
    """
    :param corpus: a list of list of tokens (generated from textClean.pipeline)
    :return: show the word cloud plot
    """
    preprocessed_tokens = corpus
    tokens = []
    for i in range(len(preprocessed_tokens)):
        tokens.extend(preprocessed_tokens[i])
    for i in range(len(tokens)):
        tokens[i] = tokens[i].lower()
    comment_words = ''
    comment_words += " ".join(tokens) + " "
    stopwords = set(STOPWORDS)
    wordcloud = WordCloud(width=800, height=800,
                          background_color='white',
                          collocations=False,
                          stopwords=stopwords,
                          min_font_size=10).generate(comment_words)
    # plot the WordCloud image
    plt.figure(figsize=(8, 8), facecolor=None)
    plt.imshow(wordcloud)
    plt.axis("off")
    plt.tight_layout(pad=0)
    plt.show()


def generate_chinese_word_cloud(text, img_link, font_path, stopwords = '',
                                color_control = False, contour_color = 'white', save = False):
    """

    :param text: corpus with space as split
    :param img_link: background image path
    :param font_path: chinese font path (e.g. simsun.ttf)
    :param stopwords: stopwords list
    :param color_control: True: use same color as background, false: use default color
    :param contour_color: color name of contour, default is white, can be changed to e.g. 'green','pink',etc
    :param save: to save img to current folder or not, default is false
    :return:
    """
    backgroud = np.array(Image.open(img_link))

    wc = WordCloud(width=800, height=800,
            background_color='white',
            mode='RGB',
            mask=backgroud, #添加蒙版，生成指定形状的词云，并且词云图的颜色可从蒙版里提取
            contour_width=3,
            contour_color=contour_color,
            max_words=500,
            stopwords=STOPWORDS.add(stopwords),#内置的屏蔽词,并添加自己设置的词语
            font_path=font_path,
            max_font_size=150,
            relative_scaling=0.6, #设置字体大小与词频的关联程度为0.4
            random_state=50,
            scale=2
            ).generate(text)
    if color_control == True:
        image_color = ImageColorGenerator(backgroud)#设置生成词云的颜色，如去掉这两行则字体为默认颜色
        wc.recolor(color_func=image_color)

    plt.imshow(wc) #显示词云
    plt.axis('off') #关闭x,y轴
    plt.show()#显示
    if save == True:
        wc.to_file('word_cloud.jpg')
