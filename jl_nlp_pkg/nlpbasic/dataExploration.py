import numpy as np
import statistics
import nltk
from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt
import pandas as pd
from scipy.spatial.distance import cosine

def print_bow_example(text, dictionary):
    """
    :param dictionary: the dictionary generated by gensim.corpora.Dictionary(processed_docs)
    :return: print bag of words vector frequency
    """
    for i in range(len(text)):
        print("Word {} (\"{}\") appears {} time.".format(text[i][0], dictionary[text[i][0]], text[i][1]))


from scipy.spatial.distance import cosine


def get_similarity_cosin(basedata, comparedata, word_col, doc_key, topn=10, filterbase=None):
    """
    :param basedata: basedata to do comparison
    :param filterdata: compare data to do comparison
    :param key: key used to do left join e.g. ['A'], ['A','B']
    :param doc_key: index key which used to identify rows
    :param topn: rows with top n similarity value
    :param filterbase: base or compare or None, to get top n list based on base table unique value or compare table unique value or combination unique value
    :return: a dataset with row doc key and similarity value
    """
    base = basedata[:]
    base['dataid'] = 'base'
    compare = comparedata[:]
    compare['dataid'] = 'compare'
    pre_tmp = base.append(compare)

    joinT = pre_tmp.pivot(index=[doc_key, 'dataid'], columns=word_col).fillna(0).reset_index(level=[0, 1])
    joinT.columns = [doc_key, 'dataid'] + [i[1] for i in joinT.columns][2:]

    baseT = joinT[joinT.dataid == 'base'].drop(columns=['dataid'])
    baseT = baseT.set_index(doc_key)
    compareT = joinT[joinT.dataid == 'compare'].drop(columns=['dataid'])
    compareT = compareT.set_index(doc_key)

    baseindex = []
    compareindex = []
    similarity = []
    for i in range(len(baseT)):
        for j in range(len(compareT)):
            baseindex.append(baseT.index[i])
            compareindex.append(compareT.index[j])
            similarity.append(1 - cosine(baseT.iloc[[i]], compareT.iloc[[j]]))

    output = pd.DataFrame({'baseindex': baseindex,
                           'compareindex': compareindex,
                           'similarity': similarity})
    output = output[output.similarity < 1].sort_values(by=['similarity'], ascending=False)  # .head(topn)

    if filterbase == None:
        output['combineid'] = output.baseindex.apply(lambda x: str(x)) + ' ' + output.compareindex.apply(lambda x: str(x))
        output['combineid'] = output['combineid'].apply(lambda x: sorted([int(i) for i in x.split(' ')]))
        output = output[~output['combineid'].apply(pd.Series).duplicated()]
        output = output.drop(columns = ['combineid'])
    if filterbase == 'base':
        output = output.drop_duplicates('baseindex')
    elif filterbase == 'compare':
        output = output.drop_duplicates('compareindex')
    output = output.head(topn)

    return output

def text_length_summary(data, col, measure = 'max'):
    """
    :param data: dataset to be summary
    :param col: column of data that want to be measured: e.g. "review"
    :param measure: 'max', 'min', 'avg', 'median'
    :return:
    """
    measurer = np.vectorize(len)
    if measure == 'max':
        out = measurer(data[col].astype(str)).max(axis=0)
    if measure == 'min':
        out = measurer(data[col].astype(str)).min(axis=0)
    if measure == 'avg':
        out = statistics.mean(measurer(data[col].astype(str)))
    if measure == 'median':
        out = statistics.median(measurer(data[col].astype(str)))
    return out

def get_topn_freq_bow(corpus, topn = 10):
    """
    :param corpus: corpus: a list of list of tokens (generated from textClean.pipeline)
    :return: top n frequent bow list
    """
    processed_docs = corpus
    doc_list = []
    for i in range(len(processed_docs)):
        doc_list.extend(processed_docs[i])
    freq_list = nltk.FreqDist(doc_list)
    topn_freq_list = freq_list.most_common(topn)
    return topn_freq_list

def generate_word_cloud(corpus):
    """
    :param corpus: a list of list of tokens (generated from textClean.pipeline)
    :return: show the word cloud plot
    """
    preprocessed_tokens = corpus
    tokens = []
    for i in range(len(preprocessed_tokens)):
        tokens.extend(preprocessed_tokens[i])
    for i in range(len(tokens)):
        tokens[i] = tokens[i].lower()
    comment_words = ''
    comment_words += " ".join(tokens) + " "
    stopwords = set(STOPWORDS)
    wordcloud = WordCloud(width=800, height=800,
                          background_color='white',
                          collocations=False,
                          stopwords=stopwords,
                          min_font_size=10).generate(comment_words)
    # plot the WordCloud image
    plt.figure(figsize=(8, 8), facecolor=None)
    plt.imshow(wordcloud)
    plt.axis("off")
    plt.tight_layout(pad=0)
    plt.show()



